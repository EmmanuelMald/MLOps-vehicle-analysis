{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Vehicle CO 2 emissions & autonomy estimators This repo born from the curiosity of create two machine learning models that can estimate: CO 2 emissions (g/km) vehicle autonomy (km/L) of any type of vehicle based on some features such as the make, basemodel, fuel type, vehicle year, among others. In this aspect, people who wants to buy a car or is interested in this type of information can easily estimate this data, and even testing how this values changes if a piece of the car is modified. The full project were developed in Python. Notebooks are used to develop the models Scripts are used for testing The repository also contains CI/CD pipelines that were implemented using Github Actions .","title":"Introduction"},{"location":"#vehicle-co2-emissions-autonomy-estimators","text":"This repo born from the curiosity of create two machine learning models that can estimate: CO 2 emissions (g/km) vehicle autonomy (km/L) of any type of vehicle based on some features such as the make, basemodel, fuel type, vehicle year, among others. In this aspect, people who wants to buy a car or is interested in this type of information can easily estimate this data, and even testing how this values changes if a piece of the car is modified. The full project were developed in Python. Notebooks are used to develop the models Scripts are used for testing The repository also contains CI/CD pipelines that were implemented using Github Actions .","title":"Vehicle CO2 emissions &amp; autonomy estimators"},{"location":"cicd/","text":"","title":"CI/CD pipelines"},{"location":"data_preparation/","text":"Data Preparation An ETL pipeline was implemented using pandas due to the amount of information is relatively small (thousands of rows); Nevertheless, if the amount of data reach millions or billions of rows, it might be worth it to implement distributed computing with pyspark. ETL stands for Extract-Transform-Load, it refers to gather information from some data source, modify this data to convert it in a more valuable asset, and then load or save this processed data in a database or file. Data Source Both machine learning models uses information from the All Vehicles Model dataset from OpenDataSoft . This dataset has licence of public domain , and is constantly being updated with the most recent vehicle models. Up to the time this documentation is being written (September 2024), the most recent model year is 2024, and the last time this dataset was updated was in June 3, 2024. Nevertheless, this project has been configured to retrieve the last version of this dataset each time that the notebooks are run. The dataset has some column names that might not be so self-descriptive Nevertheless, this dataset has some metadata that can be used to understand the columns, in this case, a json file was created to store the metadata, which can be consulted here . Data Transformation Some of the processes that were implemented are: Basic Data Understanding Data understanding is not a data transformation process itself, but I consider important to understand the basics of each column, some of the questions answered in this process were: Does a column has the necessary not null values to be set as a parameter for the ML model? In this section, if the column has a lot of missing values - more than 80%, then I wasn't used Is the information in that column meaningful for the model? (This is a first approach, but It will be addressed in depth during feature selection) Are the column information easily retrievable/known to set it as a parameter to the models? Its important to define parameters that can truly be known Using the metadata and some basic Exploratory Data Analysis, some columns of the dataset were chosen to create the ML models. Changing the data type of some columns There were some columns that should be used as numbers, or bool values, but the dataset was trating them as strings or viceversa. In that case, its important to set the right values to the columns to avoid errors. Data imputation Is the process in which some missing data is filled with values according to the data. Use this method carefully, as this can modify the data distribution. There are different methods for data imputation based on the type of the data and its context. For this project, some of the methods implemented were: KNNImputer: This imputation model takes all the other features as inputs for a KNN model, where the target is the column that is trying to be imputed. For each null row, it sets the mean value of the K nearest neighbors. This model, even when is used to set either categorical or numerical values, needs to work with numbers, so converting all the categorical features into numeric ones is mandatory. All the features (numeric and categorical) were encoded using the OrdinalEncoder. Setting a value that represents the missing data In this case, 'False', '0', and 'None', are some of the values used on the different types of missing data. Dropping duplicates Even when each row in the dataset has an unique ID, some columns that will be used by the model has the same values, so this rows were dropped. Data Load Once the data was cleaned, the resulting dataset were saved as vehicle_data_prepared.parquet . Some of the advantages of the parquet extension over the csv or xslx extension is that the parquet file uses a column storage process rather than csv and xslx, that uses row storage process. This is a more efficient way to store data as it can retrieve only some columns instead of the whole dataset if needed. Also, parquet works better for large amount of data.","title":"Data preparation"},{"location":"data_preparation/#data-preparation","text":"An ETL pipeline was implemented using pandas due to the amount of information is relatively small (thousands of rows); Nevertheless, if the amount of data reach millions or billions of rows, it might be worth it to implement distributed computing with pyspark. ETL stands for Extract-Transform-Load, it refers to gather information from some data source, modify this data to convert it in a more valuable asset, and then load or save this processed data in a database or file.","title":"Data Preparation"},{"location":"data_preparation/#data-source","text":"Both machine learning models uses information from the All Vehicles Model dataset from OpenDataSoft . This dataset has licence of public domain , and is constantly being updated with the most recent vehicle models. Up to the time this documentation is being written (September 2024), the most recent model year is 2024, and the last time this dataset was updated was in June 3, 2024. Nevertheless, this project has been configured to retrieve the last version of this dataset each time that the notebooks are run. The dataset has some column names that might not be so self-descriptive Nevertheless, this dataset has some metadata that can be used to understand the columns, in this case, a json file was created to store the metadata, which can be consulted here .","title":"Data Source"},{"location":"data_preparation/#data-transformation","text":"Some of the processes that were implemented are: Basic Data Understanding Data understanding is not a data transformation process itself, but I consider important to understand the basics of each column, some of the questions answered in this process were: Does a column has the necessary not null values to be set as a parameter for the ML model? In this section, if the column has a lot of missing values - more than 80%, then I wasn't used Is the information in that column meaningful for the model? (This is a first approach, but It will be addressed in depth during feature selection) Are the column information easily retrievable/known to set it as a parameter to the models? Its important to define parameters that can truly be known Using the metadata and some basic Exploratory Data Analysis, some columns of the dataset were chosen to create the ML models. Changing the data type of some columns There were some columns that should be used as numbers, or bool values, but the dataset was trating them as strings or viceversa. In that case, its important to set the right values to the columns to avoid errors. Data imputation Is the process in which some missing data is filled with values according to the data. Use this method carefully, as this can modify the data distribution. There are different methods for data imputation based on the type of the data and its context. For this project, some of the methods implemented were: KNNImputer: This imputation model takes all the other features as inputs for a KNN model, where the target is the column that is trying to be imputed. For each null row, it sets the mean value of the K nearest neighbors. This model, even when is used to set either categorical or numerical values, needs to work with numbers, so converting all the categorical features into numeric ones is mandatory. All the features (numeric and categorical) were encoded using the OrdinalEncoder. Setting a value that represents the missing data In this case, 'False', '0', and 'None', are some of the values used on the different types of missing data. Dropping duplicates Even when each row in the dataset has an unique ID, some columns that will be used by the model has the same values, so this rows were dropped.","title":"Data Transformation"},{"location":"data_preparation/#data-load","text":"Once the data was cleaned, the resulting dataset were saved as vehicle_data_prepared.parquet . Some of the advantages of the parquet extension over the csv or xslx extension is that the parquet file uses a column storage process rather than csv and xslx, that uses row storage process. This is a more efficient way to store data as it can retrieve only some columns instead of the whole dataset if needed. Also, parquet works better for large amount of data.","title":"Data Load"},{"location":"deployment/","text":"Model Deployment","title":"Model deployment"},{"location":"deployment/#model-deployment","text":"","title":"Model Deployment"},{"location":"machine_learning/","text":"Machine Learning Models Python was fully used for the development of this project, the libraries pandas : Data Extraction & Transformation numpy : Data Transformation scikit-learn : Model development joblib : Save the trained model seaborn : Data Visualization matplotlib : Data Visualization","title":"Machine Learning models"},{"location":"machine_learning/#machine-learning-models","text":"Python was fully used for the development of this project, the libraries pandas : Data Extraction & Transformation numpy : Data Transformation scikit-learn : Model development joblib : Save the trained model seaborn : Data Visualization matplotlib : Data Visualization","title":"Machine Learning Models"}]}