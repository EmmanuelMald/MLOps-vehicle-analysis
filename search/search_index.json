{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Vehicle CO 2 emissions & autonomy estimators This repo born from the curiosity of create two machine learning models that can estimate: CO 2 emissions (g/km) vehicle autonomy (km/L) of any type of vehicle based on some features such as the make, basemodel, fuel type, vehicle year, among others. In this aspect, people who wants to buy a car or is interested in this type of information can easily estimate this data, and even testing how this values changes if a piece of the car is modified. The full project were developed in Python. Notebooks are used to develop the models Scripts are used for testing The repository also contains CI/CD pipelines that were implemented using Github Actions .","title":"Introduction"},{"location":"#vehicle-co2-emissions-autonomy-estimators","text":"This repo born from the curiosity of create two machine learning models that can estimate: CO 2 emissions (g/km) vehicle autonomy (km/L) of any type of vehicle based on some features such as the make, basemodel, fuel type, vehicle year, among others. In this aspect, people who wants to buy a car or is interested in this type of information can easily estimate this data, and even testing how this values changes if a piece of the car is modified. The full project were developed in Python. Notebooks are used to develop the models Scripts are used for testing The repository also contains CI/CD pipelines that were implemented using Github Actions .","title":"Vehicle CO2 emissions &amp; autonomy estimators"},{"location":"autonomy_estimator/","text":"Autonomy Estimator Feature Selection All the features used was the ones with a MI (mutual information) score higer than 0.3: basemodel : Basemodel of the vehicle (ex: Forte, F-150, Mustang, etc). engine_displacement_liters : Total volume that displace the cylinders (ex. 2, 4, 6, 8, etc) cylinders : Number of cylinders of a vehicle. (ex: 3, 4, 8, etc) transmission : Type of transmission. See all the transmissions here (ex: Automatic 3-spd, Manual 4-spd, etc) vehicle_size_class : Vehicle segment (ex: Compact Cars, Large Cars, Small Sport Utility Vehicle 2WD, etc) The engine_displacement_liters and cylinders has a strong negative linear relation with the target variable ( combined_kmpl_for_fuel_type1 ), with a spearman correlation lower than -0.8 Model implemented Considering that the target variable is continous, and the high linear relationship between engine_displacement_liters and cylinders . It was decided to implement a Multiple Linear Regression and test its ability to predict new values. Feature Encoding In this case, as all the categories does not have any order/numeric relation, all of them were encoded using OneHotEncoder due to avoids possible relationships between categories that does not exist and might be created using OrdinalEncoder , especially when working with multiple linear regression, which is very sensitive to the order/hierarchy of the numbers. Feature Normalizacion Also, as multiple linear regression is sensitive to the ranges of the features, it was decided to use MinMaxScaler to normalize all the numeric values in a range between 0 and 1. As all the categorical features also have values between 0 and 1 due to OneHotEncoder, it wasn't necessary to normalize them. Model Evaluation Using cross validation, the R2 was about 0.94, and the RMSE was near 1.11 km/L, making a very precise model. It wasn't necessary to create another model.","title":"Autonomy estimator"},{"location":"autonomy_estimator/#autonomy-estimator","text":"","title":"Autonomy Estimator"},{"location":"autonomy_estimator/#feature-selection","text":"All the features used was the ones with a MI (mutual information) score higer than 0.3: basemodel : Basemodel of the vehicle (ex: Forte, F-150, Mustang, etc). engine_displacement_liters : Total volume that displace the cylinders (ex. 2, 4, 6, 8, etc) cylinders : Number of cylinders of a vehicle. (ex: 3, 4, 8, etc) transmission : Type of transmission. See all the transmissions here (ex: Automatic 3-spd, Manual 4-spd, etc) vehicle_size_class : Vehicle segment (ex: Compact Cars, Large Cars, Small Sport Utility Vehicle 2WD, etc) The engine_displacement_liters and cylinders has a strong negative linear relation with the target variable ( combined_kmpl_for_fuel_type1 ), with a spearman correlation lower than -0.8","title":"Feature Selection"},{"location":"autonomy_estimator/#model-implemented","text":"Considering that the target variable is continous, and the high linear relationship between engine_displacement_liters and cylinders . It was decided to implement a Multiple Linear Regression and test its ability to predict new values.","title":"Model implemented"},{"location":"autonomy_estimator/#feature-encoding","text":"In this case, as all the categories does not have any order/numeric relation, all of them were encoded using OneHotEncoder due to avoids possible relationships between categories that does not exist and might be created using OrdinalEncoder , especially when working with multiple linear regression, which is very sensitive to the order/hierarchy of the numbers.","title":"Feature Encoding"},{"location":"autonomy_estimator/#feature-normalizacion","text":"Also, as multiple linear regression is sensitive to the ranges of the features, it was decided to use MinMaxScaler to normalize all the numeric values in a range between 0 and 1. As all the categorical features also have values between 0 and 1 due to OneHotEncoder, it wasn't necessary to normalize them.","title":"Feature Normalizacion"},{"location":"autonomy_estimator/#model-evaluation","text":"Using cross validation, the R2 was about 0.94, and the RMSE was near 1.11 km/L, making a very precise model. It wasn't necessary to create another model.","title":"Model Evaluation"},{"location":"co2_estimator/","text":"CO 2 emission estimator Feature Selection All the features used was the ones with a MI (Mutual Information) score higher than 0.2, this is: basemodel : Basemodel of the vehicle (ex: Forte, F-150, Mustang, etc). engine_displacement_liters : Total volume that displace the cylinders (ex. 2, 4, 6, 8, etc) transmission : Type of transmission. See all the transmissions here (ex: Automatic 3-spd, Manual 4-spd, etc) make : The company or brand that manufactured a vehicle (ex: Lamborghini, Ford, Kia, etc) cylinders : Number of cylinders of a vehicle. (ex: 3, 4, 8, etc) vehicle_size_class : Vehicle segment (ex: Compact Cars, Large Cars, Small Sport Utility Vehicle 2WD, etc) year : Model year of the vehicle (ex: 2024, 2020, etc) drive : Vehicle's drivetrain (ex: Rear-Wheel Drive, 4-Wheel Drive, etc) fuel_type : Generic Fuel type, its a category that mix fuel_type1 and fuel_type2 (ex: Premium, Regular, Premium and Electricity, Regular Gas and Electricity, Hydrogen, etc) electric_motor : Type of electric motor (ex: 240V Li-Ion, 48V Li-Ion, 82 kW AC Induction, 81 kW AC PMSM, etc) fuel_type1 : The main or principal fuel that a vehicle uses (ex: Diesel, Regular Gasoline, Premium Gasoline, Electricity, etc) start_stop : If vehicle has start-stop technology (ex: True, False) One important first conclusion is that, even when there exists hybrid vehicles, the second fuel type (fuel_type2) is not a good predictor of the CO 2 emissions. It was also found that the engine_displacement_liters and the cylinders features had a strong linear relationship with the target variable ( co2_tailpipe_gpkm ). With a spearman and pearson correlation higher than 0.8 Model implemented Considering that the target variable is continous, and the high linear relationship between engine_displacement_liters and cylinders . It was decided to implement a Multiple Linear Regression and test its ability to predict new values. Feature Encoding In this case, as all the categories does not have any order/numeric relation, all of them were encoded using OneHotEncoder due to avoids possible relationships between categories that does not exist and might be created using OrdinalEncoder , especially when working with multiple linear regression, which is very sensitive to the order/hierarchy of the numbers. *In case a category feature has an ordered labels, then those should be encoded using OrdinalEncoder, which would respect the hierarchy of the labels setting higher numbers to them. Feature Normalizacion Also, as multiple linear regression is sensitive to the ranges of the features, it was decided to use MinMaxScaler to normalize all the numeric values in a range between 0 and 1. As all the categorical features also have values between 0 and 1 due to OneHotEncoder, it wasn't necessary to normalize them. Model Evaluation Using cross validation, the mean R2 was about 0.95, and RMSE near 17 g/km, making it a very precise model. So it wasn't necessary to create another model.","title":"CO2 emissions estimator"},{"location":"co2_estimator/#co2-emission-estimator","text":"","title":"CO2 emission estimator"},{"location":"co2_estimator/#feature-selection","text":"All the features used was the ones with a MI (Mutual Information) score higher than 0.2, this is: basemodel : Basemodel of the vehicle (ex: Forte, F-150, Mustang, etc). engine_displacement_liters : Total volume that displace the cylinders (ex. 2, 4, 6, 8, etc) transmission : Type of transmission. See all the transmissions here (ex: Automatic 3-spd, Manual 4-spd, etc) make : The company or brand that manufactured a vehicle (ex: Lamborghini, Ford, Kia, etc) cylinders : Number of cylinders of a vehicle. (ex: 3, 4, 8, etc) vehicle_size_class : Vehicle segment (ex: Compact Cars, Large Cars, Small Sport Utility Vehicle 2WD, etc) year : Model year of the vehicle (ex: 2024, 2020, etc) drive : Vehicle's drivetrain (ex: Rear-Wheel Drive, 4-Wheel Drive, etc) fuel_type : Generic Fuel type, its a category that mix fuel_type1 and fuel_type2 (ex: Premium, Regular, Premium and Electricity, Regular Gas and Electricity, Hydrogen, etc) electric_motor : Type of electric motor (ex: 240V Li-Ion, 48V Li-Ion, 82 kW AC Induction, 81 kW AC PMSM, etc) fuel_type1 : The main or principal fuel that a vehicle uses (ex: Diesel, Regular Gasoline, Premium Gasoline, Electricity, etc) start_stop : If vehicle has start-stop technology (ex: True, False) One important first conclusion is that, even when there exists hybrid vehicles, the second fuel type (fuel_type2) is not a good predictor of the CO 2 emissions. It was also found that the engine_displacement_liters and the cylinders features had a strong linear relationship with the target variable ( co2_tailpipe_gpkm ). With a spearman and pearson correlation higher than 0.8","title":"Feature Selection"},{"location":"co2_estimator/#model-implemented","text":"Considering that the target variable is continous, and the high linear relationship between engine_displacement_liters and cylinders . It was decided to implement a Multiple Linear Regression and test its ability to predict new values.","title":"Model implemented"},{"location":"co2_estimator/#feature-encoding","text":"In this case, as all the categories does not have any order/numeric relation, all of them were encoded using OneHotEncoder due to avoids possible relationships between categories that does not exist and might be created using OrdinalEncoder , especially when working with multiple linear regression, which is very sensitive to the order/hierarchy of the numbers. *In case a category feature has an ordered labels, then those should be encoded using OrdinalEncoder, which would respect the hierarchy of the labels setting higher numbers to them.","title":"Feature Encoding"},{"location":"co2_estimator/#feature-normalizacion","text":"Also, as multiple linear regression is sensitive to the ranges of the features, it was decided to use MinMaxScaler to normalize all the numeric values in a range between 0 and 1. As all the categorical features also have values between 0 and 1 due to OneHotEncoder, it wasn't necessary to normalize them.","title":"Feature Normalizacion"},{"location":"co2_estimator/#model-evaluation","text":"Using cross validation, the mean R2 was about 0.95, and RMSE near 17 g/km, making it a very precise model. So it wasn't necessary to create another model.","title":"Model Evaluation"},{"location":"data_preparation/","text":"Data Preparation An ETL pipeline was implemented using pandas due to the amount of information is relatively small (thousands of rows); Nevertheless, if the amount of data reach millions or billions of rows, it might be worth it to implement distributed computing with pyspark. ETL stands for Extract-Transform-Load, it refers to gather information from some data source, modify this data to convert it in a more valuable asset, and then load or save this processed data in a database or file. Data Source Both machine learning models uses information from the All Vehicles Model dataset from OpenDataSoft . This dataset has licence of public domain , and is constantly being updated with the most recent vehicle models. Up to the time this documentation is being written (September 2024), the most recent model year is 2024, and the last time this dataset was updated was in June 3, 2024. Nevertheless, this project has been configured to retrieve the last version of this dataset each time that the notebooks are run. The dataset has some column names that might not be so self-descriptive Nevertheless, this dataset has some metadata that can be used to understand the columns, in this case, a json file was created to store the metadata, which can be consulted here . Data Transformation Some of the processes that were implemented are: <<<<<<< HEAD Basic Data Understanding ======= Basic Data Understanding bd5c342f535b4d36b9a7cf75f46e949980ed4208 Data understanding is not a data transformation process itself, but I consider important to understand the basics of each column, some of the questions answered in this process were: Does a column has the necessary not null values to be set as a parameter for the ML model? In this section, if the column has a lot of missing values - more than 80%, then I wasn't used <<<<<<< HEAD - Is the information in that column meaningful for the model? (This is a first approach, but It will be addressed in depth during feature selection - described in the Machine learning models section) Are the column information easily retrievable/known to set it as a parameter to the models? Its important to define parameters that can truly be known and set as entries of the model This first analysis was achievable using the metadata and some basic Exploratory Data Analysis. Changing Data Types There were some columns that should be used as numbers, or bool values, but the dataset was trating them as strings or viceversa. In that case, its important to set the right values to the columns to avoid errors. Data Imputation ======= - Is the information in that column meaningful for the model? (This is a first approach, but It will be addressed in depth during feature selection) Are the column information easily retrievable/known to set it as a parameter to the models? Its important to define parameters that can truly be known Using the metadata and some basic Exploratory Data Analysis, some columns of the dataset were chosen to create the ML models. Changing the data type of some columns There were some columns that should be used as numbers, or bool values, but the dataset was trating them as strings or viceversa. In that case, its important to set the right values to the columns to avoid errors. Data imputation bd5c342f535b4d36b9a7cf75f46e949980ed4208 Is the process in which some missing data is filled with values according to the data. Use this method carefully, as this can modify the data distribution. There are different methods for data imputation based on the type of the data and its context. For this project, some of the methods implemented were: KNNImputer: This imputation model takes all the other features as inputs for a KNN model, where the target is the column that is trying to be imputed. For each null row, it sets the mean value of the K nearest neighbors. This model, even when is used to set either categorical or numerical values, needs to work with numbers, so converting all the categorical features into numeric ones is mandatory. All the features (numeric and categorical) were encoded using the OrdinalEncoder. Setting a value that represents the missing data In this case, 'False', '0', and 'None', are some of the values used on the different types of missing data. <<<<<<< HEAD Dropping Duplicates Even when each row in the dataset has an unique ID, some columns that will be used by the model has the same values, so this rows were dropped. Changing the Unit of Measure Some columns were originally measure using the english system of units. Nevertheless, to have a more intuitive vision of the units, the International System of Units was used. To do so, some transformations had to be done. Renaming Columns As the original dataset didn't have intuitive column names, it was decided to change those names to more intuitive ones. ======= Dropping duplicates Even when each row in the dataset has an unique ID, some columns that will be used by the model has the same values, so this rows were dropped. bd5c342f535b4d36b9a7cf75f46e949980ed4208 Data Load Once the data was cleaned, the resulting dataset were saved as vehicle_data_prepared.parquet . Some of the advantages of the parquet extension over the csv or xslx extension is that the parquet file uses a column storage process rather than csv and xslx, that uses row storage process. This is a more efficient way to store data as it can retrieve only some columns instead of the whole dataset if needed. Also, parquet works better for large amount of data.","title":"Data preparation"},{"location":"data_preparation/#data-preparation","text":"An ETL pipeline was implemented using pandas due to the amount of information is relatively small (thousands of rows); Nevertheless, if the amount of data reach millions or billions of rows, it might be worth it to implement distributed computing with pyspark. ETL stands for Extract-Transform-Load, it refers to gather information from some data source, modify this data to convert it in a more valuable asset, and then load or save this processed data in a database or file.","title":"Data Preparation"},{"location":"data_preparation/#data-source","text":"Both machine learning models uses information from the All Vehicles Model dataset from OpenDataSoft . This dataset has licence of public domain , and is constantly being updated with the most recent vehicle models. Up to the time this documentation is being written (September 2024), the most recent model year is 2024, and the last time this dataset was updated was in June 3, 2024. Nevertheless, this project has been configured to retrieve the last version of this dataset each time that the notebooks are run. The dataset has some column names that might not be so self-descriptive Nevertheless, this dataset has some metadata that can be used to understand the columns, in this case, a json file was created to store the metadata, which can be consulted here .","title":"Data Source"},{"location":"data_preparation/#data-transformation","text":"Some of the processes that were implemented are: <<<<<<< HEAD","title":"Data Transformation"},{"location":"data_preparation/#basic-data-understanding","text":"======= Basic Data Understanding bd5c342f535b4d36b9a7cf75f46e949980ed4208 Data understanding is not a data transformation process itself, but I consider important to understand the basics of each column, some of the questions answered in this process were: Does a column has the necessary not null values to be set as a parameter for the ML model? In this section, if the column has a lot of missing values - more than 80%, then I wasn't used <<<<<<< HEAD - Is the information in that column meaningful for the model? (This is a first approach, but It will be addressed in depth during feature selection - described in the Machine learning models section) Are the column information easily retrievable/known to set it as a parameter to the models? Its important to define parameters that can truly be known and set as entries of the model This first analysis was achievable using the metadata and some basic Exploratory Data Analysis.","title":"Basic Data Understanding"},{"location":"data_preparation/#changing-data-types","text":"There were some columns that should be used as numbers, or bool values, but the dataset was trating them as strings or viceversa. In that case, its important to set the right values to the columns to avoid errors.","title":"Changing Data Types"},{"location":"data_preparation/#data-imputation","text":"======= - Is the information in that column meaningful for the model? (This is a first approach, but It will be addressed in depth during feature selection) Are the column information easily retrievable/known to set it as a parameter to the models? Its important to define parameters that can truly be known Using the metadata and some basic Exploratory Data Analysis, some columns of the dataset were chosen to create the ML models. Changing the data type of some columns There were some columns that should be used as numbers, or bool values, but the dataset was trating them as strings or viceversa. In that case, its important to set the right values to the columns to avoid errors. Data imputation bd5c342f535b4d36b9a7cf75f46e949980ed4208 Is the process in which some missing data is filled with values according to the data. Use this method carefully, as this can modify the data distribution. There are different methods for data imputation based on the type of the data and its context. For this project, some of the methods implemented were: KNNImputer: This imputation model takes all the other features as inputs for a KNN model, where the target is the column that is trying to be imputed. For each null row, it sets the mean value of the K nearest neighbors. This model, even when is used to set either categorical or numerical values, needs to work with numbers, so converting all the categorical features into numeric ones is mandatory. All the features (numeric and categorical) were encoded using the OrdinalEncoder. Setting a value that represents the missing data In this case, 'False', '0', and 'None', are some of the values used on the different types of missing data. <<<<<<< HEAD","title":"Data Imputation"},{"location":"data_preparation/#dropping-duplicates","text":"Even when each row in the dataset has an unique ID, some columns that will be used by the model has the same values, so this rows were dropped.","title":"Dropping Duplicates"},{"location":"data_preparation/#changing-the-unit-of-measure","text":"Some columns were originally measure using the english system of units. Nevertheless, to have a more intuitive vision of the units, the International System of Units was used. To do so, some transformations had to be done.","title":"Changing the Unit of Measure"},{"location":"data_preparation/#renaming-columns","text":"As the original dataset didn't have intuitive column names, it was decided to change those names to more intuitive ones. ======= Dropping duplicates Even when each row in the dataset has an unique ID, some columns that will be used by the model has the same values, so this rows were dropped. bd5c342f535b4d36b9a7cf75f46e949980ed4208","title":"Renaming Columns"},{"location":"data_preparation/#data-load","text":"Once the data was cleaned, the resulting dataset were saved as vehicle_data_prepared.parquet . Some of the advantages of the parquet extension over the csv or xslx extension is that the parquet file uses a column storage process rather than csv and xslx, that uses row storage process. This is a more efficient way to store data as it can retrieve only some columns instead of the whole dataset if needed. Also, parquet works better for large amount of data.","title":"Data Load"},{"location":"deployment/","text":"Model Deployment","title":"Model deployment"},{"location":"deployment/#model-deployment","text":"","title":"Model Deployment"},{"location":"machine_learning/","text":"Machine Learning Models Both machine learning models were developed in Python; the main libraries used were: pandas : Data Extraction & Transformation numpy : Data Transformation scikit-learn : Machine Learning development joblib : Save the trained ML model Feature Selection To choose the columns that will be used to train the model, a technique called Mutual Information was used. This technique is similar to correlation in the aspect that compares the relation between two variables; Nevertheless, Mutual Information can detect any kind of relation, not only linear as correlation does. Mutual Information (MI) is a concept from information theory that measures the amount of information shared between two random variables. It quantifies how much knowing one variable reduces the uncertainty about the other. In other words, it tells you how much information about one variable is gained by knowing the value of the other variable. MI goes from 0 to infinity, where 0 indicates no relation at all; In practice, MI values between 0.1 and 0.5 often indicate a moderate relationship, while values above 0.5 suggest a stronger relationship. As MI is is scale-dependent, you could choose the top K MI values, or the ones higher than some threshold. MI only supports numeric values; so all the categorical features had to be encoded using an Ordinal Encoder. Also, a pearson and a spearman correlation was implemented to know which columns had a linear or near-linear correlation with the target. Spearman correlation is used to catch those near-linear relationships, whereas pearson is only capable to find linear relationships. Model Development To select which models to tests, the first thing to take into account is to know the type of the target and the objective you want to achieve. If the objective is to find relations between your data that has not being discovered or labeled, this is, you do not have a built target (Y), then you should try with Unsupervised ML models. On the other hand, if you want to label a new data entry in an already labeled dataset, then you should try with Supervised ML models. Another thing to take into account, is the data type of your target. If the target is a category, then you should try with classification models. Whether your data is numeric, then you should try with some regression ones. Its also important to establish if you want to create a parametric or non-parametric model, this is, if you want to create a model based on some assumptions of the relations between your features and your target, or not. Based on this, you should try with some neural networks or not. In this project, both targets, CO 2 emissions and autonomy are numeric and continous, so supervised machine learning models, and specifically some regression models, are used . Feature Encoding Once the models has been selected, its necessary to encode the categorical features based on whether the labels are ordered or unordered, as all the ML models needs to work with numbers. In this project, OneHotEncoder was used for unordered categorical features, and OrdinalEncoder for the ordered ones (if exists). Feature Normalization As almost every regression model is very sensitive to the order/hierarchy of the numbers, and the range of values of all the features are different, it was decided to use MinMaxScaler to set them all in the same range (0 to 1), avoiding modifying its distributions. Pipeline Generation As some transformations needs to be done to all the features in the cleaned dataset before being used by any Machine Learning model, ColumnTransformer was used to select which type of transformation (encoder/scaler) needs to be done to each feature; moreover, it can also be integrated with Pipeline , which orchestrates all the steps to be done to use the model, making it easier to generate a model that can only receive the \"raw\" dataset values, and return the estimation. Model Evaluation To evaluate the models, cross validation (cross_val_score) was used to prevent overfitting and ensuring that the model performs well on unseen data. It evaluates the model's performance by training it on different subsets of the data and testing it on the remaining parts. This approach provides a more reliable estimate of a model's performance compared to a single train-test split, ensuring that the model's evaluation isn't dependent on any specific data partition. In sklearn, cross validation allows to implement different evaluation metrics. In this project, two was used: R2 This metric tells how much of the variation of the data can be described by the model. R2 goes from 0 to 1. The nearest to 1, the better the model is. Root Mean Squared Error (RMSE) It measures the average magnitude of the errors between predicted and actual values. It provides an idea of how well a model's predictions match the observed data. Lower values means a better model performance. Since RMSE has the same unit as the target variable, it's easy to interpret as the mean deviation to the real value.","title":"Tools implemented"},{"location":"machine_learning/#machine-learning-models","text":"Both machine learning models were developed in Python; the main libraries used were: pandas : Data Extraction & Transformation numpy : Data Transformation scikit-learn : Machine Learning development joblib : Save the trained ML model","title":"Machine Learning Models"},{"location":"machine_learning/#feature-selection","text":"To choose the columns that will be used to train the model, a technique called Mutual Information was used. This technique is similar to correlation in the aspect that compares the relation between two variables; Nevertheless, Mutual Information can detect any kind of relation, not only linear as correlation does. Mutual Information (MI) is a concept from information theory that measures the amount of information shared between two random variables. It quantifies how much knowing one variable reduces the uncertainty about the other. In other words, it tells you how much information about one variable is gained by knowing the value of the other variable. MI goes from 0 to infinity, where 0 indicates no relation at all; In practice, MI values between 0.1 and 0.5 often indicate a moderate relationship, while values above 0.5 suggest a stronger relationship. As MI is is scale-dependent, you could choose the top K MI values, or the ones higher than some threshold. MI only supports numeric values; so all the categorical features had to be encoded using an Ordinal Encoder. Also, a pearson and a spearman correlation was implemented to know which columns had a linear or near-linear correlation with the target. Spearman correlation is used to catch those near-linear relationships, whereas pearson is only capable to find linear relationships.","title":"Feature Selection"},{"location":"machine_learning/#model-development","text":"To select which models to tests, the first thing to take into account is to know the type of the target and the objective you want to achieve. If the objective is to find relations between your data that has not being discovered or labeled, this is, you do not have a built target (Y), then you should try with Unsupervised ML models. On the other hand, if you want to label a new data entry in an already labeled dataset, then you should try with Supervised ML models. Another thing to take into account, is the data type of your target. If the target is a category, then you should try with classification models. Whether your data is numeric, then you should try with some regression ones. Its also important to establish if you want to create a parametric or non-parametric model, this is, if you want to create a model based on some assumptions of the relations between your features and your target, or not. Based on this, you should try with some neural networks or not. In this project, both targets, CO 2 emissions and autonomy are numeric and continous, so supervised machine learning models, and specifically some regression models, are used .","title":"Model Development"},{"location":"machine_learning/#feature-encoding","text":"Once the models has been selected, its necessary to encode the categorical features based on whether the labels are ordered or unordered, as all the ML models needs to work with numbers. In this project, OneHotEncoder was used for unordered categorical features, and OrdinalEncoder for the ordered ones (if exists).","title":"Feature Encoding"},{"location":"machine_learning/#feature-normalization","text":"As almost every regression model is very sensitive to the order/hierarchy of the numbers, and the range of values of all the features are different, it was decided to use MinMaxScaler to set them all in the same range (0 to 1), avoiding modifying its distributions.","title":"Feature Normalization"},{"location":"machine_learning/#pipeline-generation","text":"As some transformations needs to be done to all the features in the cleaned dataset before being used by any Machine Learning model, ColumnTransformer was used to select which type of transformation (encoder/scaler) needs to be done to each feature; moreover, it can also be integrated with Pipeline , which orchestrates all the steps to be done to use the model, making it easier to generate a model that can only receive the \"raw\" dataset values, and return the estimation.","title":"Pipeline Generation"},{"location":"machine_learning/#model-evaluation","text":"To evaluate the models, cross validation (cross_val_score) was used to prevent overfitting and ensuring that the model performs well on unseen data. It evaluates the model's performance by training it on different subsets of the data and testing it on the remaining parts. This approach provides a more reliable estimate of a model's performance compared to a single train-test split, ensuring that the model's evaluation isn't dependent on any specific data partition. In sklearn, cross validation allows to implement different evaluation metrics. In this project, two was used:","title":"Model Evaluation"},{"location":"machine_learning/#r2","text":"This metric tells how much of the variation of the data can be described by the model. R2 goes from 0 to 1. The nearest to 1, the better the model is.","title":"R2"},{"location":"machine_learning/#root-mean-squared-error-rmse","text":"It measures the average magnitude of the errors between predicted and actual values. It provides an idea of how well a model's predictions match the observed data. Lower values means a better model performance. Since RMSE has the same unit as the target variable, it's easy to interpret as the mean deviation to the real value.","title":"Root Mean Squared Error (RMSE)"}]}